{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69kCBmfOEInQ"
      },
      "source": [
        "#importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdDVWeW9dPgn",
        "outputId": "0f547f23-b465-4e96-f231-64135413ba21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VveNf5cA13Y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install community\n",
        "!pip install python-louvain\n",
        "!pip install tsia\n",
        "!pip install networkx\n",
        "!pip install easydev\n",
        "!pip install colormap\n",
        "!pip install tomaster\n",
        "!pip install karateclub\n",
        "!pip uninstall umap\n",
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIQBOfFgceFD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import seaborn as sns\n",
        "\n",
        "from matplotlib import gridspec\n",
        "from numba import njit, prange\n",
        "from pyts.image import MarkovTransitionField\n",
        "\n",
        "import tsia.plot\n",
        "import tsia.markov\n",
        "import tsia.network_graph\n",
        "\n",
        "import community\n",
        "from community import community_louvain\n",
        "import networkx as nx\n",
        "\n",
        "from matplotlib.colors import to_hex\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "from scipy.stats import kurtosis, skew\n",
        "\n",
        "import csv\n",
        "from colormap import rgb2hex\n",
        "from tomaster import tomato\n",
        "\n",
        "from karateclub import MUSAE\n",
        "from karateclub import GraphWave\n",
        "from karateclub import Role2Vec\n",
        "from karateclub import Diff2Vec\n",
        "\n",
        "import seaborn as sns\n",
        "from pickle import FALSE\n",
        "import scipy\n",
        "import scipy.sparse as sparse\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uICSv_sdENWc"
      },
      "source": [
        "#funcs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKbzzXUHFiPJ"
      },
      "outputs": [],
      "source": [
        "# Useful constants definition\n",
        "COLORMAP = 'jet'\n",
        "\n",
        "def get_network_graph2(mtf):\n",
        "    # Build the graph with networkx:\n",
        "    graph = nx.from_numpy_matrix(mtf)\n",
        "    \n",
        "    # Loops through the edges to get associate each of them with the\n",
        "    # corresponding Markov transition probability:\n",
        "    weights = [mtf[u,v] for u,v in graph.edges()]\n",
        "    for index, e in enumerate(graph.edges()):\n",
        "        graph[e[0]][e[1]]['weight'] = weights[index]\n",
        "        \n",
        "    return graph\n",
        "    \n",
        "def compute_network_graph_statistics2(partitions, graph=None, mtf=None):    \n",
        "    if (graph is None) and (mtf is not None):\n",
        "        graph = get_network_graph(mtf)\n",
        "        \n",
        "    #partitions = community_louvain.best_partition(graph, random_state=1234)\n",
        "    nb_partitions = len(set(partitions.values()))\n",
        "    modularity = community_louvain.modularity(partitions, graph)\n",
        "\n",
        "    \n",
        "    diameter = nx.diameter(graph)\n",
        "    node_size = list(nx.clustering(graph, weight='weight').values())\n",
        "    avg_clustering_coeff = np.array(node_size).mean()\n",
        "    density = nx.density(graph)\n",
        "    avg_path_length = nx.average_shortest_path_length(graph, weight='weight', method='dijkstra')\n",
        "    \n",
        "    average_degree = nx.average_degree_connectivity(graph)\n",
        "    average_degree = np.mean(list(average_degree.values()))\n",
        "    avg_weighted_degree = nx.average_degree_connectivity(graph, weight='weight')\n",
        "    avg_weighted_degree = np.mean(list(avg_weighted_degree.values()))\n",
        "    \n",
        "    statistics = {\n",
        "        'Diameter': diameter,\n",
        "        'Average degree': average_degree,\n",
        "        'Average weighted degree': avg_weighted_degree,\n",
        "        'Density': density,\n",
        "        'Average path length': avg_path_length,\n",
        "        'Average clustering coefficient': avg_clustering_coeff,\n",
        "        'Modularity': modularity,\n",
        "        'Partitions': nb_partitions\n",
        "    }\n",
        "    \n",
        "    return statistics\n",
        "    \n",
        "def get_modularity_encoding2(graph, colormap=COLORMAP, reversed_cmap=False):\n",
        "  \n",
        "    if reversed_cmap == True:\n",
        "        colormap = plt.cm.get_cmap(colormap).reversed()\n",
        "    else:\n",
        "        colormap = plt.cm.get_cmap(colormap)\n",
        "    \n",
        "    # Get the node partitions and number of partitions found with the Louvain\n",
        "    # algorithm, as implemented in the `community` package:\n",
        "\n",
        "    partitions = community_louvain.best_partition(graph, random_state=1234)\n",
        "    #####################################\n",
        "    \n",
        "    nb_partitions = len(set(partitions.values()))\n",
        "    #print(\"nb_partitions: \",nb_partitions)\n",
        "\n",
        "    # Compute node colors and edges colors for the modularity encoding:\n",
        "    edge_colors = [to_hex(colormap(partitions.get(v)/(nb_partitions - 1))) for u,v in graph.edges()]\n",
        "    node_colors = [partitions.get(node) for node in graph.nodes()]\n",
        "    node_size = list(nx.clustering(graph, weight='weight').values())\n",
        "    node_size = list((node_size - np.min(node_size)) * 2000 + 10)\n",
        "    \n",
        "    # Store the encoding to return in a dictionnary:\n",
        "    #print(\"node_colors: \",len(set(node_colors)))\n",
        "\n",
        "    encoding = {\n",
        "        'node_size': node_size,\n",
        "        'edge_color': edge_colors,\n",
        "        'node_color': node_colors\n",
        "    }\n",
        "    return encoding, partitions\n",
        "\n",
        "def foo(w,m,v):\n",
        "  x2=[]\n",
        "  x3=[]\n",
        "  x4=[]\n",
        "  n=len(w)\n",
        "  for j in range(n):\n",
        "    x2.append(v[j]+m[j]**2)\n",
        "    x3.append(pow(m[j],3)+3*m[j]*v[j])\n",
        "    x4.append(pow(m[j],4)+6*m[j]**2*v[j]+3*v[j]**2)\n",
        "  X1=np.dot(w,m)\n",
        "  X2=np.dot(w,x2)\n",
        "  X3=np.dot(w,x3)\n",
        "  X4=np.dot(w,x4)\n",
        "  mu=X1\n",
        "  sig=np.sqrt(np.subtract(X2, mu**2))\n",
        "  sk=(X3-3*X2*X1+2*pow(X1,3))/pow(sig,3)\n",
        "  kur=(X4-4*X3*X1+6*X2*X1**2-3*pow(X1,4))/pow(sig,4)\n",
        "  return [mu, sig, sk, kur]\n",
        "\n",
        "def get_network_graph_map2(timeseries, encoding, colormap=COLORMAP, reversed_cmap=False):\n",
        "   \n",
        "    # Get encoding definitions:\n",
        "    node_colors = encoding['node_color']\n",
        "\n",
        "    #print(node_colors)\n",
        "\n",
        "    image_size = len(node_colors)\n",
        "    #print(\"node_colors\",node_colors)\n",
        "    #print(\"np.max(node_colors)\",np.max(node_colors))\n",
        "    partition_color = node_colors / np.max(node_colors)\n",
        "\n",
        "    # Define the color map:\n",
        "    if reversed_cmap == True:\n",
        "        colormap = plt.cm.get_cmap(colormap).reversed()\n",
        "    else:\n",
        "        colormap = plt.cm.get_cmap(colormap)\n",
        "\n",
        "    # Plot each subset of the signal with the color associated to the network\n",
        "    # graph partition it belongs to:\n",
        "    network_graph_map = []\n",
        "    sequences_width = timeseries.shape[0] / image_size\n",
        "\n",
        "    #df=pd.DataFrame([{\"color\": p ,\"value\": k}])\n",
        "\n",
        "    for i in range(image_size):\n",
        "        c = colormap(partition_color[i])\n",
        "\n",
        "        start = int(i * sequences_width)\n",
        "        end = int((i+1) * sequences_width)#-1\n",
        "        data = timeseries.iloc[start:end, :]\n",
        "\n",
        "        current_map = dict()\n",
        "\n",
        "        current_map.update({\n",
        "            'color': c,\n",
        "            'slice': data\n",
        "        })\n",
        "\n",
        "        #print(len(current_map[\"slice\"]))\n",
        "\n",
        "        network_graph_map.append(current_map)\n",
        "        \n",
        "    return network_graph_map, node_colors\n",
        "\n",
        "\n",
        "def inversemapAna(ng_map2,colors2):\n",
        "\n",
        "  df=pd.DataFrame(columns=[\"color\",\"value\"])\n",
        "  dout=pd.DataFrame(columns=[\"color\",\"value\"])\n",
        "\n",
        "  #if (len(ng_map2)!=len(colors2)):\n",
        "  #    print(\"ERROR\")\n",
        "\n",
        "  for i in range(len(ng_map2)):\n",
        "      d=ng_map2[i]\n",
        "      p=colors2[i]\n",
        "      slic=d[\"slice\"].values.reshape(-1)\n",
        "\n",
        "      for k in slic:\n",
        "        df=df.append([{\"color\": p ,\"value\": k}], ignore_index=True)\n",
        "  \n",
        "  df[\"diff\"]=df[\"value\"]-df[\"value\"].shift(1)\n",
        "  df.drop(df.index[[0]], inplace=True)\n",
        "  df.drop(['value'], axis = 1, inplace=True)\n",
        "  df.rename(columns = {'diff':'value'}, inplace = True)\n",
        "  #print(df)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRZq3w0j8Ts_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEf8_aa_V-r4"
      },
      "source": [
        "#Embedding-nearest exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd1euLBCWGIf"
      },
      "outputs": [],
      "source": [
        "hubnames=[\"pjm\",\"sp15\",\"paloverde\",\"nepool\"]\n",
        "strategies=[\"quantile\",\"normal\"]\n",
        "tag_data = pd.read_pickle(\"/content/depuratated_log_prices.pk\")\n",
        "r = pd.read_pickle(\"/content/log_returns.pk\")\n",
        "klist=range(2,101,1)\n",
        "modelName=\"NEU_ASNE\"\n",
        "clus=pd.DataFrame()\n",
        "returns=pd.DataFrame()\n",
        "\n",
        "for hub in hubnames:\n",
        "  for strategy in strategies:\n",
        "    path = r\"/content/drive/MyDrive/Mari/Paper3/Embed2/\"+modelName+\"/\"+hub+\"/\"#+strategy+\"/\"+hub\n",
        "\n",
        "    directories = os.listdir( path )\n",
        "    flag=True\n",
        "\n",
        "    for file in directories:\n",
        "      #print(file)\n",
        "      if (file.find(strategy)> -1):\n",
        "        embedding=np.loadtxt(path+file)\n",
        "        bins=int(file.split(\"_\")[2].replace(\".txt\",\"\"))\n",
        "\n",
        "        for k in klist:\n",
        "          if (bins>0):\n",
        "            \n",
        "            scaler = StandardScaler()\n",
        "            embedding=scaler.fit_transform(embedding)\n",
        "\n",
        "            clusters = tomato(points=embedding, k=k)\n",
        "            num_clusters=len(set(clusters))\n",
        "\n",
        "            clus=pd.DataFrame(clusters,columns=[\"cluster\"])\n",
        "            #clus.drop(clus.index[0], inplace=True)#\n",
        "            colors=list(range(0,num_clusters))\n",
        "            returns=pd.DataFrame(r[hub])\n",
        "            returns.index -= 1 #added\n",
        "            #returnCluster=pd.merge(clus, returns, on=clus.index, how=\"inner\")\n",
        "            returnCluster=clus.join(returns)\n",
        "            print(bins)\n",
        "\n",
        "            if (num_clusters<=10 and (1 not in returnCluster.cluster.value_counts().values) ):\n",
        "              tag_df=returns#tag_data[hub]\n",
        "              tag_df=pd.DataFrame(tag_df)\n",
        "\n",
        "              means=[]\n",
        "              precisions=[]\n",
        "              nk=[]\n",
        "              \n",
        "              for x in colors:        \n",
        "                a=returnCluster[hub][returnCluster.cluster==x]\n",
        "                means.append(np.mean(a))\n",
        "                precisions.append(1/pow(np.std(a),2))\n",
        "                nk.append(len(a)/(len(returnCluster)))##controllare     \n",
        "                \n",
        "              precisions=np.array(precisions).reshape(-1,1,1)\n",
        "              means=np.array(means).reshape(-1,1)\n",
        "\n",
        "              grid=pd.DataFrame(columns=[\"idxs\",\"hub\",\"aic\",\"bic\",\"comp\",\"weights\",\"means\",\"covariances\"])\n",
        "              grid.set_index(\"idxs\")     \n",
        "              \n",
        "              itemorig={\"1\":tag_df[hub].mean(),\n",
        "                    \"2\":tag_df[hub].std(),\n",
        "                    \"3\":skew(tag_df[hub]),\n",
        "                    \"4\":kurtosis(tag_df[hub])+3\n",
        "              }\n",
        "\n",
        "              XY = tag_df[hub].values.reshape(-1, 1)#tag_data\n",
        "              \n",
        "              gmm = GaussianMixture(n_components=len(nk), weights_init=nk, means_init=means, precisions_init=precisions, covariance_type='full').fit(XY)\n",
        "        \n",
        "              nosim=foo(gmm.weights_.reshape(-1),gmm.means_.reshape(-1),gmm.covariances_.reshape(-1))\n",
        "\n",
        "              grid.at[0,'comp']=num_clusters\n",
        "              grid.at[0,'hub']=hub\n",
        "              grid.at[0,'comp']=gmm.n_components\n",
        "\n",
        "              grid.at[0,'bins']=bins\n",
        "              grid.at[0,'k']=k\n",
        "              #grid.at[0,'netstat']=str(\"\")\n",
        "\n",
        "              grid.at[0,'bic']=gmm.bic(XY) \n",
        "              grid.at[0,'aic']=gmm.aic(XY)\n",
        "              grid.at[0,'weights']=gmm.weights_.reshape(-1)\n",
        "              grid.at[0,'means']=gmm.means_.reshape(-1)\n",
        "              grid.at[0,'covariances']=gmm.covariances_.reshape(-1)\n",
        "\n",
        "              grid.at[0,'orig_M1']=itemorig[\"1\"]\n",
        "              \n",
        "              grid.at[0,'orig_M2']=itemorig[\"2\"]\n",
        "              grid.at[0,'orig_M3']=itemorig[\"3\"]\n",
        "              grid.at[0,'orig_M4']=itemorig[\"4\"]\n",
        "\n",
        "              grid.at[0,'GMM_M1']=nosim[0]\n",
        "              \n",
        "              grid.at[0,'GMM_M2']=nosim[1]\n",
        "              grid.at[0,'GMM_M3']=nosim[2]\n",
        "              grid.at[0,'GMM_M4']=nosim[3]\n",
        "\n",
        "              grid.at[0,'absdiff_M1']=abs(itemorig[\"1\"]-nosim[0])\n",
        "              \n",
        "              grid.at[0,'absdiff_M2']=abs(itemorig[\"2\"]-nosim[1])\n",
        "              grid.at[0,'absdiff_M3']=abs(itemorig[\"3\"]-nosim[2])\n",
        "              grid.at[0,'absdiff_M4']=abs(itemorig[\"4\"]-nosim[3])\n",
        "\n",
        "              grid.at[0,'rel%diff_M1']=100*abs((itemorig[\"1\"]-nosim[0])/(itemorig[\"1\"]))\n",
        "              \n",
        "              grid.at[0,'rel%diff_M2']=100*abs((itemorig[\"2\"]-nosim[1])/(itemorig[\"2\"]))\n",
        "              grid.at[0,'rel%diff_M3']=100*abs((itemorig[\"3\"]-nosim[2])/(itemorig[\"3\"]))\n",
        "              grid.at[0,'rel%diff_M4']=100*abs((itemorig[\"4\"]-nosim[3])/(itemorig[\"4\"]))\n",
        "              \n",
        "              if (flag):\n",
        "                grid.to_csv(\"/content/drive/MyDrive/Mari/Paper3/Results/\"+modelName+\"/\"+strategy+\"_\"+hub+\".csv\", header=True, mode='a')\n",
        "                flag=False\n",
        "              else:      \n",
        "                grid.to_csv(\"/content/drive/MyDrive/Mari/Paper3/Results/\"+modelName+\"/\"+strategy+\"_\"+hub+\".csv\", header=False, mode='a')\n",
        "\n",
        "              print(bins,\"#\",k)\n",
        "              print(\"####\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4zRzIvF4DCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uICSv_sdENWc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}